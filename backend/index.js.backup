import express from 'express';
import multer from 'multer';
import axios from 'axios';
import cors from 'cors';
import dotenv from 'dotenv';
import fs from 'fs/promises';
import fsSync from 'fs';
import path from 'path';
import crypto from 'crypto';
import FormData from 'form-data';
import compression from 'compression';
import { validateCorsOrigin, sanitizeFileName, validateGitHubUrl } from './utils/security.js';
import { 
  AppError, 
  errorHandler, 
  asyncHandler,
  createValidationError,
  createTimeoutError,
  createExternalServiceError,
  ErrorTypes
} from './utils/errorHandler.js';

dotenv.config();

const app = express();

// Enhanced logging
const logLevel = process.env.LOG_LEVEL || 'info';
const logger = {
  info: (msg) => logLevel !== 'silent' && console.log(`[INFO] ${new Date().toISOString()} - ${msg}`),
  warn: (msg) => logLevel !== 'silent' && console.warn(`[WARN] ${new Date().toISOString()} - ${msg}`),
  error: (msg) => console.error(`[ERROR] ${new Date().toISOString()} - ${msg}`)
};

// Middleware setup
app.use(compression());
app.use(express.json({ limit: process.env.JSON_LIMIT || '20mb' }));

// Enhanced CORS configuration with proper security
const allowedOrigins = (process.env.ALLOWED_ORIGINS || 'http://localhost:3000,http://localhost:3001,http://localhost:3002')
  .split(',')
  .map(s => s.trim())
  .filter(Boolean);

logger.info(`Allowed CORS origins: ${allowedOrigins.join(', ')}`);

app.use(cors({
  origin: (origin, callback) => {
    const isAllowed = validateCorsOrigin(origin, allowedOrigins);
    
    if (isAllowed) {
      callback(null, true);
    } else {
      logger.warn(`CORS rejected origin: ${origin || 'null'}`);
      callback(new Error(`CORS policy violation: Origin ${origin || 'null'} not allowed`), false);
    }
  },
  credentials: process.env.CORS_CREDENTIALS === 'true',
  methods: ['GET', 'POST', 'OPTIONS'],
  allowedHeaders: ['Content-Type', 'Authorization']
}));

const upload = multer({
  dest: 'uploads/',
  limits: { fileSize: Number(process.env.UPLOAD_LIMIT_BYTES || 50 * 1024 * 1024) } // 50MB default
});

// Base URL for python-parser service (default to localhost when not in Docker)
const PYTHON_PARSER_URL = process.env.PYTHON_PARSER_URL || 'http://localhost:5000';

// Simple in-memory cache for analyze results (reset on server restart)
const cache = new Map(); // key: cacheKey(url, commit?), value: { data, ts }
const MAX_CACHE_ENTRIES = Number(process.env.MAX_CACHE_ENTRIES || 200);
const ensureCacheCapacity = () => {
  while (cache.size > MAX_CACHE_ENTRIES) {
    const firstKey = cache.keys().next().value;
    if (!firstKey) break;
    cache.delete(firstKey);
  }
};
const CACHE_TTL_MS = Number(process.env.CACHE_TTL_MS || 5 * 60 * 1000);

// Persistent disk cache (optional) - using async operations
const DISK_CACHE_DIR = process.env.DISK_CACHE_DIR || path.join(process.cwd(), 'cache');
const DISK_CACHE_TTL_MS = Number(process.env.DISK_CACHE_TTL_MS || 24 * 60 * 60 * 1000); // 24h

// Initialize cache directory asynchronously
fs.mkdir(DISK_CACHE_DIR, { recursive: true }).catch(error => {
  logger.warn(`Cache directory creation failed: ${error.message}`);
});

const diskKey = (key) => crypto.createHash('sha1').update(key).digest('hex');

const readDiskCache = async (key) => {
  try {
    const file = path.join(DISK_CACHE_DIR, `${diskKey(key)}.json`);
    const stat = await fs.stat(file);
    
    if ((Date.now() - stat.mtimeMs) > DISK_CACHE_TTL_MS) {
      // File expired, remove it
      await fs.unlink(file).catch(() => {}); // Ignore errors
      return null;
    }
    
    const content = await fs.readFile(file, 'utf8');
    return JSON.parse(content);
  } catch (error) {
    return null; // File doesn't exist or parse error
  }
};

const writeDiskCache = async (key, data) => {
  try {
    const file = path.join(DISK_CACHE_DIR, `${diskKey(key)}.json`);
    await fs.writeFile(file, JSON.stringify(data));
  } catch (error) {
    logger.warn(`Disk cache write failed: ${error.message}`);
  }
};

const cacheKey = (url, commit) => commit ? `${url}@${commit}` : url;

// Allow tests to inject a mock HTTP client
let http = axios;
export const __setHttpClient = (client) => { http = client || axios; };

// /analyze endpoint with enhanced security and async operations
app.post('/analyze', upload.single('repoZip'), asyncHandler(async (req, res) => {
  const { githubUrl } = req.body;
  
  if (githubUrl) {
    // Validate GitHub URL
    const urlValidation = validateGitHubUrl(githubUrl);
    if (!urlValidation.isValid) {
      throw createValidationError(urlValidation.error);
    }
    
    const now = Date.now();
    const key = cacheKey(githubUrl);
    
    // Check memory cache first
    let cached = cache.get(key);
    if (cached && (now - cached.ts) < CACHE_TTL_MS) {
      logger.info(`Cache hit (memory) for: ${githubUrl}`);
      return res.status(200).json({ success: true, ...cached.data });
    }
    
    // Check disk cache
    let disk = await readDiskCache(key);
    if (disk) {
      logger.info(`Cache hit (disk) for: ${githubUrl}`);
      cache.set(key, { data: disk, ts: Date.now() });
      ensureCacheCapacity();
      return res.status(200).json({ success: true, ...disk });
    }
    
    logger.info(`Analyzing repository: ${githubUrl}`);
    
    try {
      const response = await http.post(`${PYTHON_PARSER_URL}/analyze`, 
        { githubUrl: urlValidation.url }, 
        { timeout: Number(process.env.ANALYZE_TIMEOUT_MS || 300000) }
      );
      
      // Store cache on success only
      if (response.status >= 200 && response.status < 300) {
        const commit = response.data?.meta?.commit;
        const commitKey = cacheKey(githubUrl, commit);
        const urlKey = cacheKey(githubUrl);
        
        // Store under commit-specific key
        cache.set(commitKey, { data: response.data, ts: Date.now() });
        // Also store/update URL-only alias for quick hits
        cache.set(urlKey, { data: response.data, ts: Date.now() });
        ensureCacheCapacity();
        
        // Async disk cache writes (don't block response)
        writeDiskCache(commitKey, response.data).catch(err => 
          logger.warn(`Disk cache write failed: ${err.message}`)
        );
        writeDiskCache(urlKey, response.data).catch(err => 
          logger.warn(`Disk cache write failed: ${err.message}`)
        );
        
        logger.info(`Analysis complete for: ${githubUrl}`);
      }
      
      return res.status(response.status).json({ success: true, ...response.data });
      
    } catch (axiosError) {
      // Convert axios errors to appropriate AppErrors
      if (axiosError.code === 'ECONNABORTED' || axiosError.code === 'ETIMEDOUT') {
        throw createTimeoutError('Repository analysis');
      }
      
      if (axiosError.response) {
        const status = axiosError.response.status;
        const data = axiosError.response.data;
        
        if (status >= 400 && status < 500) {
          throw createValidationError(data?.error || `Analysis failed: ${status}`);
        } else {
          throw createExternalServiceError('Python parser', axiosError);
        }
      }
      
      throw createExternalServiceError('Python parser', axiosError);
    }
    
  } else if (req.file) {
    // Handle file upload securely
    const sanitizedName = sanitizeFileName(req.file.originalname || 'repo.zip');
    logger.info(`Processing uploaded file: ${sanitizedName}`);
    
    const form = new FormData();
    form.append('repoZip', fsSync.createReadStream(req.file.path), sanitizedName);
    
    try {
      const response = await http.post(`${PYTHON_PARSER_URL}/analyze`, form, {
        headers: form.getHeaders(),
        maxBodyLength: Infinity,
        maxContentLength: Infinity,
        timeout: Number(process.env.ANALYZE_TIMEOUT_MS || 300000),
      });
      
      // Cleanup temp upload file asynchronously
      fs.unlink(req.file.path).catch(err => 
        logger.warn(`Temp file cleanup failed: ${err.message}`)
      );
      
      return res.status(response.status).json({ success: true, ...response.data });
      
    } catch (axiosError) {
      // Clean up uploaded file on error
      fs.unlink(req.file.path).catch(() => {}); // Ignore cleanup errors
      
      // Convert axios errors to appropriate AppErrors
      if (axiosError.code === 'ECONNABORTED' || axiosError.code === 'ETIMEDOUT') {
        throw createTimeoutError('File analysis');
      }
      
      if (axiosError.response) {
        const status = axiosError.response.status;
        const data = axiosError.response.data;
        
        if (status >= 400 && status < 500) {
          throw createValidationError(data?.error || `File analysis failed: ${status}`);
        } else {
          throw createExternalServiceError('Python parser', axiosError);
        }
      }
      
      throw createExternalServiceError('Python parser', axiosError);
    }
    
  } else {
    throw createValidationError('No repository provided', {
      expected: 'githubUrl or repoZip file',
      received: 'neither'
    });
  }
}));
  
  try {
    if (githubUrl) {
      // Validate GitHub URL
      const urlValidation = validateGitHubUrl(githubUrl);
      if (!urlValidation.isValid) {
        return res.status(400).json({ error: urlValidation.error });
      }
      
      const now = Date.now();
      const key = cacheKey(githubUrl);
      
      // Check memory cache first
      let cached = cache.get(key);
      if (cached && (now - cached.ts) < CACHE_TTL_MS) {
        logger.info(`Cache hit (memory) for: ${githubUrl}`);
        return res.status(200).json(cached.data);
      }
      
      // Check disk cache
      let disk = await readDiskCache(key);
      if (disk) {
        logger.info(`Cache hit (disk) for: ${githubUrl}`);
        cache.set(key, { data: disk, ts: Date.now() });
        ensureCacheCapacity();
        return res.status(200).json(disk);
      }
      
      logger.info(`Analyzing repository: ${githubUrl}`);
      
      const response = await http.post(`${PYTHON_PARSER_URL}/analyze`, 
        { githubUrl: urlValidation.url }, 
        { timeout: Number(process.env.ANALYZE_TIMEOUT_MS || 300000) }
      );
      
      // Store cache on success only
      if (response.status >= 200 && response.status < 300) {
        const commit = response.data?.meta?.commit;
        const commitKey = cacheKey(githubUrl, commit);
        const urlKey = cacheKey(githubUrl);
        
        // Store under commit-specific key
        cache.set(commitKey, { data: response.data, ts: Date.now() });
        // Also store/update URL-only alias for quick hits
        cache.set(urlKey, { data: response.data, ts: Date.now() });
        ensureCacheCapacity();
        
        // Async disk cache writes (don't block response)
        writeDiskCache(commitKey, response.data).catch(err => 
          logger.warn(`Disk cache write failed: ${err.message}`)
        );
        writeDiskCache(urlKey, response.data).catch(err => 
          logger.warn(`Disk cache write failed: ${err.message}`)
        );
        
        logger.info(`Analysis complete for: ${githubUrl}`);
      }
      
      return res.status(response.status).json(response.data);
      
    } else if (req.file) {
      // Handle file upload securely
      const sanitizedName = sanitizeFileName(req.file.originalname || 'repo.zip');
      logger.info(`Processing uploaded file: ${sanitizedName}`);
      
      const form = new FormData();
      form.append('repoZip', fsSync.createReadStream(req.file.path), sanitizedName);
      
      const response = await http.post(`${PYTHON_PARSER_URL}/analyze`, form, {
        headers: form.getHeaders(),
        maxBodyLength: Infinity,
        maxContentLength: Infinity,
        timeout: Number(process.env.ANALYZE_TIMEOUT_MS || 300000),
      });
      
      // Cleanup temp upload file asynchronously
      fs.unlink(req.file.path).catch(err => 
        logger.warn(`Temp file cleanup failed: ${err.message}`)
      );
      
      return res.status(response.status).json(response.data);
      
    } else {
      return res.status(400).json({ 
        error: 'No repository provided',
        details: 'Please provide either githubUrl or upload a repoZip file'
      });
// Enhanced health endpoint with dependency checks
app.get('/health', async (req, res) => {
  const health = {
    status: 'healthy',
    timestamp: new Date().toISOString(),
    version: '1.0.0',
    services: {
      pythonParser: {
        url: PYTHON_PARSER_URL,
        status: 'unknown'
      },
      cache: {
        memoryEntries: cache.size,
        maxEntries: MAX_CACHE_ENTRIES,
        diskPath: DISK_CACHE_DIR
      }
    }
  };

  // Check Python parser connectivity
  try {
    const response = await http.get(`${PYTHON_PARSER_URL}/health`, { timeout: 5000 });
    health.services.pythonParser.status = response.status === 200 ? 'healthy' : 'unhealthy';
  } catch (error) {
    health.services.pythonParser.status = 'unreachable';
    health.services.pythonParser.error = error.message;
    health.status = 'degraded';
  }

  const statusCode = health.status === 'healthy' ? 200 : 503;
  res.status(statusCode).json(health);
});

// Add error handling middleware
app.use(errorHandler(logger));

// Export app for testing
export default app;

// Only start server if run directly (simple ESM check)
if (process.env.NODE_ENV !== 'test' && process.argv[1] && process.argv[1].toLowerCase().endsWith('index.js')) {
  const port = process.env.PORT || 3001;
  const server = app.listen(port, () => {
    logger.info(`ðŸš€ Backend server started on port ${port}`);
    logger.info(`ðŸ“¡ Python parser URL: ${PYTHON_PARSER_URL}`);
    logger.info(`ðŸ—‚ï¸  Cache directory: ${DISK_CACHE_DIR}`);
    logger.info(`ðŸ”’ CORS origins: ${allowedOrigins.join(', ')}`);
  });

  // Graceful shutdown handling
  const gracefulShutdown = (signal) => {
    logger.info(`\nðŸ›‘ ${signal} received. Starting graceful shutdown...`);
    
    server.close(() => {
      logger.info('âœ… HTTP server closed');
      
      // Clear cache and cleanup
      cache.clear();
      logger.info('âœ… Memory cache cleared');
      
      process.exit(0);
    });

    // Force close after 10 seconds
    setTimeout(() => {
      logger.error('âŒ Forced shutdown after timeout');
      process.exit(1);
    }, 10000);
  };

  process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
  process.on('SIGINT', () => gracefulShutdown('SIGINT'));
}
